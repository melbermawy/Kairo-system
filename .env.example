########################################
# CORE APP CONFIG  (PR-0 / PR-1)
########################################

# local | test | staging | prod
APP_ENV=local

# django mandatory secret. use a long random string in real envs.
DJANGO_SECRET_KEY=dev-change-me-please-very-long-random-string

# True for local dev only.
DJANGO_DEBUG=True

# canonical base url your backend thinks it lives at
# (used later for links / callbacks; safe to keep dummy for PRD-1)
BACKEND_BASE_URL=http://localhost:8000


########################################
# DATABASE / SUPABASE  (PR-0 / PR-1)
########################################

# primary connection string used by django
# pick **one** style and stick to it; this is the real source of truth.
DATABASE_URL=postgresql://kairo:kairo@localhost:5432/kairo

# if you want to keep “supabase-ness” explicit, you can also expose:
SUPABASE_DB_URL=postgresql://kairo:kairo@localhost:5432/kairo

# in the future, if you use supabase auth/storage directly from django:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_ROLE_KEY=your-service-role-key


########################################
# LLM / OPENAI  (WIRED IN PR-7+, USED HEAVILY PR-8+)
########################################

# openai api key – required once we turn LLM on.
OPENAI_API_KEY=sk-...

# model names - used by kairo.hero.llm_client
# GPT-5.x models use Responses API; older models use Chat Completions API.
# Valid GPT-5 models: gpt-5-nano, gpt-5-mini, gpt-5, gpt-5-pro, gpt-5.1, gpt-5.1-mini
#
# "fast" model for cheaper utility tasks (scoring, classification)
KAIRO_LLM_MODEL_FAST=gpt-5-nano
# "heavy" model for hero flows, content, deep reasoning
KAIRO_LLM_MODEL_HEAVY=gpt-5-pro

# global kill switch for live LLM calls.
# for PRD-1 and CI this should stay True and we'll use a fake client.
LLM_DISABLED=False

# timeouts (seconds)
KAIRO_LLM_TIMEOUT_FAST=8
KAIRO_LLM_TIMEOUT_HEAVY=20

# max output tokens
KAIRO_LLM_MAX_TOKENS_FAST=1024
KAIRO_LLM_MAX_TOKENS_HEAVY=4096

# sampling parameters (deterministic by default for eval reproducibility)
KAIRO_LLM_TEMP_FAST=0.0
KAIRO_LLM_TEMP_HEAVY=0.0


########################################
# DEEPAGENTS / ORCHESTRATION  (PR-8+)
########################################

# mostly for logging / toggles; not strictly required for PRD-1.
DEEPAGENTS_ENV=local

# knob to disable specific graphs in emergencies (comma-sep names)
DISABLED_GRAPHS=


########################################
# LOGGING / OBSERVABILITY  (PR-6 / PR-11)
########################################

# python logging level: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=DEBUG

# optional sentry (or similar) – empty means disabled.
SENTRY_DSN=

# include run_id in logs (we’ll wire this in PR-6)
INCLUDE_RUN_ID_IN_LOGS=True


########################################
# EVAL / QA  (PR-10+)
########################################

# directory where eval artifacts are dumped; must match evalHarness doc
EVAL_OUTPUT_DIR=./eval/hero_loop/out

# in CI, eval harness must run in mocked-LLM mode even if LLM_DISABLED=False
EVAL_FORCE_FAKE_LLM=True


########################################
# EXTERNAL SIGNALS  (FUTURE PRDs, NOT PRD-1)
########################################

# these are **not** used in PRD-1; they’re here so contracts are ready.

# google trends / search (example, adjust to actual provider later)
GOOGLE_API_KEY=
GOOGLE_SEARCH_CX=

# twitter/x, linkedin, etc. – leave empty for now; fixtures only.
X_BEARER_TOKEN=
LINKEDIN_CLIENT_ID=
LINKEDIN_CLIENT_SECRET=


########################################
# SECURITY / CORS  (PR-0+)
########################################

# comma-separated list of origins allowed to call the backend
CORS_ALLOWED_ORIGINS=http://localhost:3000

# if you later host the UI on a different domain, add it here.